{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aditya\\Documents\\modeling-spreads\\TestingAT\\test.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/TestingAT/test.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m SelectFromModel\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/TestingAT/test.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/TestingAT/test.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mModels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_evaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/TestingAT/test.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mroot_path\u001b[39;00m \u001b[39mimport\u001b[39;00m ROOT_PATH\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/TestingAT/test.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Ignore sklearn warnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Models'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the main section at the bottom of this file to train and evaluate models for Spread Predictions,\n",
    "Points total predictions, Favorite Cover Probabilities, and Over Cover Probabilities.\n",
    "\n",
    "Inside the folder for each model in Models/XGBoost/F$dependent variable$ there will be\n",
    "3 files. ScoreStats.txt has info on RMSE and R-squared (or Accuracy, Precision, Recall, and F1 for classification).\n",
    "ScoreTrainingLoss.png contains a plot of the training losses to gauge over fitting. ScoreModel.keras is the model that\n",
    "can be loaded at any time.\n",
    "\n",
    "This script automatically tunes hyperparemeters for the final model through a grid search cross validation.\n",
    "To edit which hyperparameters are tested and the range of values for each hyperparameter tested, edit the\n",
    "`param_tuning` dictionary on lines 164-169. Grid search at the moment is set to n_jobs=-1, which uses all available\n",
    "cores on the machine. To change this, edit n_jobs in the `params_model` object (lines 176 and 180)\n",
    "to the maximum desired number of cores.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from Models.model_evaluator import *\n",
    "from root_path import ROOT_PATH\n",
    "\n",
    "# Ignore sklearn warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    \"\"\"Class to train an xgboost model\"\"\"\n",
    "\n",
    "    def __init__(self, dependent_variable):\n",
    "        self.dependent_variable = dependent_variable\n",
    "        self.significant_feature_names = None\n",
    "        self.threshold = None\n",
    "        self.best_params = None\n",
    "        self.model_folder_path = create_model_folder(\"XGBoost\", self.dependent_variable)\n",
    "\n",
    "        # Load data\n",
    "        print(\"Loading data\")\n",
    "        self.data = pd.read_csv(f\"{ROOT_PATH}/Data/expanded_data.csv\")\n",
    "        self.training_set = pd.read_csv(f\"{ROOT_PATH}/Data/SplitData/training_set.csv\")\n",
    "        self.validation_set = pd.read_csv(f\"{ROOT_PATH}/Data/SplitData/validation_set_set.csv\")\n",
    "        self.test_set = pd.read_csv(f\"{ROOT_PATH}/Data/SplitData/test_set.csv\")\n",
    "        # TODO: Define independent/dependent variables, filter dataframes below accordingly\n",
    "        self.X_train = self.training_set.drop([\"HERE\"], axis=1)\n",
    "        self.X_validation = self.validation_set.drop([\"HERE\"], axis=1)\n",
    "        self.X_test = self.test_set.drop([\"HERE\"], axis=1)\n",
    "        self.y_train = self.training_set[dependent_variable]\n",
    "        self.y_validation = self.validation_set[dependent_variable]\n",
    "        self.y_test = self.test_set[dependent_variable]\n",
    "\n",
    "    def find_feature_importance(self):\n",
    "        \"\"\"Function to plot and select most important features using the elbow method\"\"\"\n",
    "        print(\"Training XGB model with default hyperparameters to find feature importance\")\n",
    "\n",
    "        # Find feature significance\n",
    "        if self.dependent_variable == 'Spread':\n",
    "            model = XGBRegressor(random_state=33)\n",
    "        else:\n",
    "            model = XGBClassifier(random_state=33)\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Finding feature significance\")\n",
    "        thresholds = sort(list(model.feature_importances_))\n",
    "\n",
    "        # Create a model using each feature importance as a feature threshold, pick threshold with lowest RMSE\n",
    "        model_thresholds = dict()\n",
    "        file = open(f\"{self.model_folder_path}/{self.dependent_variable.replace(' ', '')}ThresholdEval.txt\", 'w')\n",
    "        counter = 1\n",
    "        # Iterate through thresholds\n",
    "        for thresh in thresholds:\n",
    "            features = SelectFromModel(estimator=model, threshold=thresh, prefit=True)\n",
    "            # Transform feature sets\n",
    "            features_X_train = features.transform(self.X_train)\n",
    "            features_X_validation = features.transform(self.X_validation)\n",
    "\n",
    "            # Fit model\n",
    "            if self.dependent_variable == 'Spread':\n",
    "                feature_model = XGBRegressor(random_state=33)\n",
    "            else:\n",
    "                feature_model = XGBClassifier(random_state=33)\n",
    "            feature_model.fit(features_X_train, self.y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            predictions = feature_model.predict(features_X_validation)\n",
    "\n",
    "            # RMSE for spread (regression\n",
    "            if self.dependent_variable == 'Spread':\n",
    "                rmse = mean_squared_error(self.y_validation, predictions, squared=False)\n",
    "                model_thresholds[thresh] = rmse\n",
    "                # Save to file\n",
    "                file.write(f\"\\n- Threshold: {thresh}  - RMSE: {rmse}\")\n",
    "                print(f\"Evaluating feature threshold: {thresh}; - RMSE: {rmse}; - PROGRESS: {counter}/{len(thresholds)}\")\n",
    "                counter += 1\n",
    "\n",
    "                file.close()\n",
    "\n",
    "                # Find threshold that has best RMSE\n",
    "                min_rmse = min(model_thresholds.values())\n",
    "                for thresh in model_thresholds:\n",
    "                    if model_thresholds[thresh] == min_rmse:\n",
    "                        self.threshold = thresh\n",
    "                        break\n",
    "                print(f\"Found best threshold is {self.threshold} with RMSE of {min_rmse}\")\n",
    "\n",
    "            # Accuracy for bet win prediction/over prediction\n",
    "            else:\n",
    "                accuracy = accuracy_score(self.y_validation, predictions)\n",
    "                model_thresholds[thresh] = accuracy\n",
    "                # Save to file\n",
    "                file.write(f\"\\n- Threshold: {thresh}  - Accuracy: {accuracy}\")\n",
    "                print(f\"Evaluating feature threshold: {thresh}; - Accuracy: {accuracy}; - PROGRESS: {counter}/{len(thresholds)}\")\n",
    "                counter += 1\n",
    "\n",
    "                file.close()\n",
    "\n",
    "                # Find threshold that has best RMSE\n",
    "                min_accuracy = min(model_thresholds.values())\n",
    "                for thresh in model_thresholds:\n",
    "                    if model_thresholds[thresh] == min_accuracy:\n",
    "                        self.threshold = thresh\n",
    "                        break\n",
    "                print(f\"Found best threshold is {self.threshold} with Accuracy of {min_accuracy}\")\n",
    "\n",
    "        print(\"Filtering data to only include selected features\")\n",
    "\n",
    "        # Use the best threshold for final feature selection\n",
    "        significant_features = SelectFromModel(model, threshold=self.threshold, prefit=True)\n",
    "        self.significant_feature_names = [self.X_train.columns[i] for i in significant_features.get_support(indices=True)]\n",
    "\n",
    "        # Save most significant features to a file\n",
    "        print(\"Writing feature names and importance to a file\")\n",
    "        with open(f\"{ROOT_PATH}/Models/FeatureSelection/{self.dependent_variable.replace(' ', '')}MostSignificantFeatures.pkl\", \"wb\") as pklfile:\n",
    "            pickle.dump(self.significant_feature_names, pklfile)\n",
    "            pklfile.close()\n",
    "\n",
    "        # Save second file with importance\n",
    "        importance_vals = model.feature_importances_\n",
    "        importance_dict = dict(sorted({model.feature_names_in_[i]: str(importance_vals[i]) for i in range(len(importance_vals)) if importance_vals[i] >= self.threshold}.items(),\n",
    "                                      key=lambda x: x[1], reverse=True))\n",
    "        with open(f\"{self.model_folder_path}/{self.dependent_variable.replace(' ', '')}MostSignificantFeatureValues.txt\", \"w\") as file:\n",
    "            file.write(str(importance_dict))\n",
    "            file.close()\n",
    "\n",
    "        # Update X_train and X_test with selected features\n",
    "        self.X_train = significant_features.transform(self.X_train)\n",
    "        self.X_validation = significant_features.transform(self.X_validation)\n",
    "        self.X_test = significant_features.transform(self.X_test)\n",
    "\n",
    "    def find_optimal_hyperparameters(self):\n",
    "        \"\"\"Function which uses cross validation to find optimal model hyperparameters\"\"\"\n",
    "        # Create xgboost D-matrices\n",
    "        print(\"Creating D-matrices and setting parameter values for cross validation\")\n",
    "        d_train = xgb.DMatrix(self.X_train, self.y_train, enable_categorical=True)\n",
    "        d_test = xgb.DMatrix(self.X_validation, self.y_validation, enable_categorical=True)\n",
    "\n",
    "        # Create dictionary of potential parameters for testing in cross validation\n",
    "        param_tuning = {\n",
    "            \"max_depth\": np.arange(3, 10),\n",
    "            \"learning_rate\": np.arange(0.1, 1, 0.1),\n",
    "            \"n_estimators\": np.arange(100, 1000, 100),\n",
    "            \"gamma\": np.arange(0, 5)\n",
    "        }\n",
    "\n",
    "        # Use grid search to perform k-fold cross validation with k=5 to find best parameters\n",
    "        print(\"Performing 5 fold cross validation:\")\n",
    "        # Create xgb object\n",
    "        if self.dependent_variable == 'Spread':\n",
    "            xgb_object = XGBRegressor(random_state=33)\n",
    "            params_model = GridSearchCV(estimator=xgb_object, param_grid=param_tuning, scoring=\"neg_mean_squared_error\",\n",
    "                                        verbose=10, n_jobs=-1)\n",
    "        else:\n",
    "            xgb_object = XGBClassifier(random_state=33)\n",
    "            params_model = GridSearchCV(estimator=xgb_object, param_grid=param_tuning, scoring=\"accuracy\",\n",
    "                                        verbose=10, n_jobs=-1)\n",
    "\n",
    "        # Find best params\n",
    "        params_model.fit(self.X_train, self.y_train)\n",
    "        self.best_params = params_model.best_params_\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Function to train an xgboost model\"\"\"\n",
    "        print(f\"Creating final {self.dependent_variable} model with best parameters from cross validation\")\n",
    "\n",
    "        # Spread model\n",
    "        if self.dependent_variable == 'Spread':\n",
    "            model = XGBRegressor(**self.best_params, random_state=33)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "            print(f\"Collecting model {self.dependent_variable} performance statistics\")\n",
    "            calculate_performance_metrics_regression('NeuralNetwork',\n",
    "                                                     self.dependent_variable,\n",
    "                                                     model,\n",
    "                                                     self.X_train,\n",
    "                                                     self.X_validation,\n",
    "                                                     self.X_test,\n",
    "                                                     self.y_train,\n",
    "                                                     self.y_validation,\n",
    "                                                     self.y_test,\n",
    "                                                     best_params=self.best_params\n",
    "                                                     )\n",
    "        # Bet win probability model\n",
    "        else:\n",
    "            model = XGBClassifier(**self.best_params, random_state=33)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "            print(f\"Collecting model {self.dependent_variable} performance statistics\")\n",
    "            calculate_performance_metrics_classification('NeuralNetwork',\n",
    "                                                         self.dependent_variable,\n",
    "                                                         model,\n",
    "                                                         self.X_train,\n",
    "                                                         self.X_validation,\n",
    "                                                         self.X_test,\n",
    "                                                         self.y_train,\n",
    "                                                         self.y_validation,\n",
    "                                                         self.y_test,\n",
    "                                                         best_params=self.best_params\n",
    "                                                         )\n",
    "\n",
    "        # Save Model\n",
    "        print(f\"Saving {self.dependent_variable} model and performance statistics\")\n",
    "        model.save_model(f\"{self.model_folder_path}/{self.dependent_variable.replace(' ', '')}XGBoostModel.json\")\n",
    "\n",
    "    def create_model(self):\n",
    "        self.find_feature_importance()\n",
    "        self.find_optimal_hyperparameters()\n",
    "        self.train_model()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #  Initialize model trainers\n",
    "    spread_trainer = XGBoostTrainer(\"Spread\")\n",
    "    favorite_cover_trainer = XGBoostTrainer(\"Favorite Cover Probability\")\n",
    "    over_trainer = XGBoostTrainer(\"Over Probability\")\n",
    "    points_total_trainer = XGBoostTrainer(\"Points Total\")\n",
    "\n",
    "    # Train BigModels\n",
    "    spread_trainer.create_model()\n",
    "    favorite_cover_trainer.create_model()\n",
    "    over_trainer.create_model()\n",
    "    points_total_trainer.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aditya\\Documents\\modeling-spreads\\TestingAT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting Models\n",
      "  Downloading models-0.9.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Aditya\\AppData\\Local\\Temp\\pip-install-tj0khcyc\\models_70b6268e362746c1b6391937eef1561b\\setup.py\", line 25, in <module>\n",
      "          import models\n",
      "        File \"C:\\Users\\Aditya\\AppData\\Local\\Temp\\pip-install-tj0khcyc\\models_70b6268e362746c1b6391937eef1561b\\models\\__init__.py\", line 23, in <module>\n",
      "          from base import *\n",
      "      ModuleNotFoundError: No module named 'base'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Aditya\\\\Documents\\\\modeling-spreads/Data/SplitData/training_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Aditya\\Documents\\modeling-spreads\\Models\\test.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=234'>235</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_model()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=237'>238</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m     \u001b[39m#  Initialize model trainers\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=239'>240</a>\u001b[0m     spread_trainer \u001b[39m=\u001b[39m XGBoostTrainer(\u001b[39m\"\u001b[39;49m\u001b[39mSpread\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=240'>241</a>\u001b[0m     favorite_cover_trainer \u001b[39m=\u001b[39m XGBoostTrainer(\u001b[39m\"\u001b[39m\u001b[39mFavorite Cover Probability\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=241'>242</a>\u001b[0m     over_trainer \u001b[39m=\u001b[39m XGBoostTrainer(\u001b[39m\"\u001b[39m\u001b[39mOver Probability\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Aditya\\Documents\\modeling-spreads\\Models\\test.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mROOT_PATH\u001b[39m}\u001b[39;00m\u001b[39m/Data/expanded_data.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_set \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mROOT_PATH\u001b[39m}\u001b[39;49;00m\u001b[39m/Data/SplitData/training_set.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_set \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mROOT_PATH\u001b[39m}\u001b[39;00m\u001b[39m/Data/SplitData/validation_set_set.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Aditya/Documents/modeling-spreads/Models/test.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_set \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mROOT_PATH\u001b[39m}\u001b[39;00m\u001b[39m/Data/SplitData/test_set.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Aditya\\\\Documents\\\\modeling-spreads/Data/SplitData/training_set.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the main section at the bottom of this file to train and evaluate models for Spread Predictions,\n",
    "Points total predictions, Favorite Cover Probabilities, and Over Cover Probabilities.\n",
    "\n",
    "Inside the folder for each model in Models/XGBoost/F$dependent variable$ there will be\n",
    "3 files. ScoreStats.txt has info on RMSE and R-squared (or Accuracy, Precision, Recall, and F1 for classification).\n",
    "ScoreTrainingLoss.png contains a plot of the training losses to gauge over fitting. ScoreModel.keras is the model that\n",
    "can be loaded at any time.\n",
    "\n",
    "This script automatically tunes hyperparemeters for the final model through a grid search cross validation.\n",
    "To edit which hyperparameters are tested and the range of values for each hyperparameter tested, edit the\n",
    "`param_tuning` dictionary on lines 164-169. Grid search at the moment is set to n_jobs=-1, which uses all available\n",
    "cores on the machine. To change this, edit n_jobs in the `params_model` object (lines 176 and 180)\n",
    "to the maximum desired number of cores.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from Models.model_evaluator import *\n",
    "from root_path import ROOT_PATH\n",
    "\n",
    "# Ignore sklearn warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "class XGBoostTrainer:\n",
    "    \"\"\"Class to train an xgboost model\"\"\"\n",
    "\n",
    "    def __init__(self, dependent_variable):\n",
    "        self.dependent_variable = dependent_variable\n",
    "        self.significant_feature_names = None\n",
    "        self.threshold = None\n",
    "        self.best_params = None\n",
    "        self.model_folder_path = create_model_folder(\"XGBoost\", self.dependent_variable)\n",
    "\n",
    "        # Load data\n",
    "        print(\"Loading data\")\n",
    "        self.data = pd.read_csv(f\"{ROOT_PATH}/Data/expanded_data.csv\")\n",
    "        self.training_set = pd.read_csv(f\"{ROOT_PATH}/Data/SplitData/training_set.csv\")\n",
    "        self.validation_set = pd.read_csv(f\"{ROOT_PATH}/Data/SplitData/validation_set_set.csv\")\n",
    "        self.test_set = pd.read_csv(f\"{ROOT_PATH}/Data/SplitData/test_set.csv\")\n",
    "        # TODO: Define independent/dependent variables, filter dataframes below accordingly\n",
    "        self.X_train = self.training_set.drop([\"HERE\"], axis=1)\n",
    "        self.X_validation = self.validation_set.drop([\"HERE\"], axis=1)\n",
    "        self.X_test = self.test_set.drop([\"HERE\"], axis=1)\n",
    "        self.y_train = self.training_set[dependent_variable]\n",
    "        self.y_validation = self.validation_set[dependent_variable]\n",
    "        self.y_test = self.test_set[dependent_variable]\n",
    "\n",
    "    def find_feature_importance(self):\n",
    "        \"\"\"Function to plot and select most important features using the elbow method\"\"\"\n",
    "        print(\"Training XGB model with default hyperparameters to find feature importance\")\n",
    "\n",
    "        # Find feature significance\n",
    "        if self.dependent_variable == 'Spread':\n",
    "            model = XGBRegressor(random_state=33)\n",
    "        else:\n",
    "            model = XGBClassifier(random_state=33)\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        print(\"Finding feature significance\")\n",
    "        thresholds = sort(list(model.feature_importances_))\n",
    "\n",
    "        # Create a model using each feature importance as a feature threshold, pick threshold with lowest RMSE\n",
    "        model_thresholds = dict()\n",
    "        file = open(f\"{self.model_folder_path}/{self.dependent_variable.replace(' ', '')}ThresholdEval.txt\", 'w')\n",
    "        counter = 1\n",
    "        # Iterate through thresholds\n",
    "        for thresh in thresholds:\n",
    "            features = SelectFromModel(estimator=model, threshold=thresh, prefit=True)\n",
    "            # Transform feature sets\n",
    "            features_X_train = features.transform(self.X_train)\n",
    "            features_X_validation = features.transform(self.X_validation)\n",
    "\n",
    "            # Fit model\n",
    "            if self.dependent_variable == 'Spread':\n",
    "                feature_model = XGBRegressor(random_state=33)\n",
    "            else:\n",
    "                feature_model = XGBClassifier(random_state=33)\n",
    "            feature_model.fit(features_X_train, self.y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            predictions = feature_model.predict(features_X_validation)\n",
    "\n",
    "            # RMSE for spread (regression\n",
    "            if self.dependent_variable == 'Spread':\n",
    "                rmse = mean_squared_error(self.y_validation, predictions, squared=False)\n",
    "                model_thresholds[thresh] = rmse\n",
    "                # Save to file\n",
    "                file.write(f\"\\n- Threshold: {thresh}  - RMSE: {rmse}\")\n",
    "                print(f\"Evaluating feature threshold: {thresh}; - RMSE: {rmse}; - PROGRESS: {counter}/{len(thresholds)}\")\n",
    "                counter += 1\n",
    "\n",
    "                file.close()\n",
    "\n",
    "                # Find threshold that has best RMSE\n",
    "                min_rmse = min(model_thresholds.values())\n",
    "                for thresh in model_thresholds:\n",
    "                    if model_thresholds[thresh] == min_rmse:\n",
    "                        self.threshold = thresh\n",
    "                        break\n",
    "                print(f\"Found best threshold is {self.threshold} with RMSE of {min_rmse}\")\n",
    "\n",
    "            # Accuracy for bet win prediction/over prediction\n",
    "            else:\n",
    "                accuracy = accuracy_score(self.y_validation, predictions)\n",
    "                model_thresholds[thresh] = accuracy\n",
    "                # Save to file\n",
    "                file.write(f\"\\n- Threshold: {thresh}  - Accuracy: {accuracy}\")\n",
    "                print(f\"Evaluating feature threshold: {thresh}; - Accuracy: {accuracy}; - PROGRESS: {counter}/{len(thresholds)}\")\n",
    "                counter += 1\n",
    "\n",
    "                file.close()\n",
    "\n",
    "                # Find threshold that has best RMSE\n",
    "                min_accuracy = min(model_thresholds.values())\n",
    "                for thresh in model_thresholds:\n",
    "                    if model_thresholds[thresh] == min_accuracy:\n",
    "                        self.threshold = thresh\n",
    "                        break\n",
    "                print(f\"Found best threshold is {self.threshold} with Accuracy of {min_accuracy}\")\n",
    "\n",
    "        print(\"Filtering data to only include selected features\")\n",
    "\n",
    "        # Use the best threshold for final feature selection\n",
    "        significant_features = SelectFromModel(model, threshold=self.threshold, prefit=True)\n",
    "        self.significant_feature_names = [self.X_train.columns[i] for i in significant_features.get_support(indices=True)]\n",
    "\n",
    "        # Save most significant features to a file\n",
    "        print(\"Writing feature names and importance to a file\")\n",
    "        with open(f\"{ROOT_PATH}/Models/FeatureSelection/{self.dependent_variable.replace(' ', '')}MostSignificantFeatures.pkl\", \"wb\") as pklfile:\n",
    "            pickle.dump(self.significant_feature_names, pklfile)\n",
    "            pklfile.close()\n",
    "\n",
    "        # Save second file with importance\n",
    "        importance_vals = model.feature_importances_\n",
    "        importance_dict = dict(sorted({model.feature_names_in_[i]: str(importance_vals[i]) for i in range(len(importance_vals)) if importance_vals[i] >= self.threshold}.items(),\n",
    "                                      key=lambda x: x[1], reverse=True))\n",
    "        with open(f\"{self.model_folder_path}/{self.dependent_variable.replace(' ', '')}MostSignificantFeatureValues.txt\", \"w\") as file:\n",
    "            file.write(str(importance_dict))\n",
    "            file.close()\n",
    "\n",
    "        # Update X_train and X_test with selected features\n",
    "        self.X_train = significant_features.transform(self.X_train)\n",
    "        self.X_validation = significant_features.transform(self.X_validation)\n",
    "        self.X_test = significant_features.transform(self.X_test)\n",
    "\n",
    "    def find_optimal_hyperparameters(self):\n",
    "        \"\"\"Function which uses cross validation to find optimal model hyperparameters\"\"\"\n",
    "        # Create xgboost D-matrices\n",
    "        print(\"Creating D-matrices and setting parameter values for cross validation\")\n",
    "        d_train = xgb.DMatrix(self.X_train, self.y_train, enable_categorical=True)\n",
    "        d_test = xgb.DMatrix(self.X_validation, self.y_validation, enable_categorical=True)\n",
    "\n",
    "        # Create dictionary of potential parameters for testing in cross validation\n",
    "        param_tuning = {\n",
    "            \"max_depth\": np.arange(3, 10),\n",
    "            \"learning_rate\": np.arange(0.1, 1, 0.1),\n",
    "            \"n_estimators\": np.arange(100, 1000, 100),\n",
    "            \"gamma\": np.arange(0, 5)\n",
    "        }\n",
    "\n",
    "        # Use grid search to perform k-fold cross validation with k=5 to find best parameters\n",
    "        print(\"Performing 5 fold cross validation:\")\n",
    "        # Create xgb object\n",
    "        if self.dependent_variable == 'Spread':\n",
    "            xgb_object = XGBRegressor(random_state=33)\n",
    "            params_model = GridSearchCV(estimator=xgb_object, param_grid=param_tuning, scoring=\"neg_mean_squared_error\",\n",
    "                                        verbose=10, n_jobs=-1)\n",
    "        else:\n",
    "            xgb_object = XGBClassifier(random_state=33)\n",
    "            params_model = GridSearchCV(estimator=xgb_object, param_grid=param_tuning, scoring=\"accuracy\",\n",
    "                                        verbose=10, n_jobs=-1)\n",
    "\n",
    "        # Find best params\n",
    "        params_model.fit(self.X_train, self.y_train)\n",
    "        self.best_params = params_model.best_params_\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Function to train an xgboost model\"\"\"\n",
    "        print(f\"Creating final {self.dependent_variable} model with best parameters from cross validation\")\n",
    "\n",
    "        # Spread model\n",
    "        if self.dependent_variable == 'Spread':\n",
    "            model = XGBRegressor(**self.best_params, random_state=33)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "            print(f\"Collecting model {self.dependent_variable} performance statistics\")\n",
    "            calculate_performance_metrics_regression('NeuralNetwork',\n",
    "                                                     self.dependent_variable,\n",
    "                                                     model,\n",
    "                                                     self.X_train,\n",
    "                                                     self.X_validation,\n",
    "                                                     self.X_test,\n",
    "                                                     self.y_train,\n",
    "                                                     self.y_validation,\n",
    "                                                     self.y_test,\n",
    "                                                     best_params=self.best_params\n",
    "                                                     )\n",
    "        # Bet win probability model\n",
    "        else:\n",
    "            model = XGBClassifier(**self.best_params, random_state=33)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            # Evaluate model\n",
    "            print(f\"Collecting model {self.dependent_variable} performance statistics\")\n",
    "            calculate_performance_metrics_classification('NeuralNetwork',\n",
    "                                                         self.dependent_variable,\n",
    "                                                         model,\n",
    "                                                         self.X_train,\n",
    "                                                         self.X_validation,\n",
    "                                                         self.X_test,\n",
    "                                                         self.y_train,\n",
    "                                                         self.y_validation,\n",
    "                                                         self.y_test,\n",
    "                                                         best_params=self.best_params\n",
    "                                                         )\n",
    "\n",
    "        # Save Model\n",
    "        print(f\"Saving {self.dependent_variable} model and performance statistics\")\n",
    "        model.save_model(f\"{self.model_folder_path}/{self.dependent_variable.replace(' ', '')}XGBoostModel.json\")\n",
    "\n",
    "    def create_model(self):\n",
    "        self.find_feature_importance()\n",
    "        self.find_optimal_hyperparameters()\n",
    "        self.train_model()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #  Initialize model trainers\n",
    "    spread_trainer = XGBoostTrainer(\"Spread\")\n",
    "    favorite_cover_trainer = XGBoostTrainer(\"Favorite Cover Probability\")\n",
    "    over_trainer = XGBoostTrainer(\"Over Probability\")\n",
    "    points_total_trainer = XGBoostTrainer(\"Points Total\")\n",
    "\n",
    "    # Train BigModels\n",
    "    spread_trainer.create_model()\n",
    "    favorite_cover_trainer.create_model()\n",
    "    over_trainer.create_model()\n",
    "    points_total_trainer.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Aditya\\\\Documents\\\\modeling-spreads\\\\Models', 'c:\\\\Program Files\\\\Python310\\\\python310.zip', 'c:\\\\Program Files\\\\Python310\\\\DLLs', 'c:\\\\Program Files\\\\Python310\\\\lib', 'c:\\\\Program Files\\\\Python310', '', 'C:\\\\Users\\\\Aditya\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages', 'C:\\\\Users\\\\Aditya\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Aditya\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Aditya\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\Pythonwin', 'c:\\\\Program Files\\\\Python310\\\\lib\\\\site-packages', 'c:\\\\Users\\\\Aditya\\\\Documents\\\\modeling-spreads']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('c:\\\\Users\\\\Aditya\\\\Documents\\\\modeling-spreads')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
